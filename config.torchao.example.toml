# Example configuration file for using torchao quantization with heretic
# Copy this file to config.toml and modify as needed

# Use torchao for quantization instead of bitsandbytes
use_torchao = true

# Type of torchao quantization to use
# Options: int4_weight_only, int8_weight_only, int8_dynamic_activation_int8_weight, 
# float8_dynamic_activation_float8_weight, float8_weight_only, autoquant
torchao_quant_type = "int4_weight_only"

# Group size for torchao weight-only quantization
torchao_group_size = 128

# Include embedding layers in torchao quantization
torchao_include_embedding = false

# Abliterate quantized model directly in GPU memory instead of loading full precision model to CPU.
# Faster but may have precision issues.
abliterate_quantized_inplace = false

# Model to use
model = "meta-llama/Llama-3.1-8B-Instruct"

# Device map (auto is recommended)
device_map = "auto"

# Other settings remain the same as the default configuration
batch_size = 0  # auto
max_batch_size = 128
max_response_length = 100
kl_divergence_scale = 1.0
n_trials = 200
n_startup_trials = 60

# System prompt
system_prompt = "You are a helpful assistant."

# Dataset configurations (same as default)
[good_prompts]
dataset = "mlabonne/harmless_alpaca"
split = "train[:400]"
column = "text"

[bad_prompts]
dataset = "mlabonne/harmful_behaviors"
split = "train[:400]"
column = "text"

[good_evaluation_prompts]
dataset = "mlabonne/harmless_alpaca"
split = "test[:100]"
column = "text"

[bad_evaluation_prompts]
dataset = "mlabonne/harmful_behaviors"
split = "test[:100]"
column = "text"